{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks with Pure Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example code for the lecture series \"Machine Learning for Physicists\" by Florian Marquardt\n",
    "\n",
    "Lecture 1\n",
    "\n",
    "See https://machine-learning-for-physicists.org and the current course website linked there!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to:\n",
    "- implement the forward-pass (evaluation) of a deep, fully connected neural network in a few lines of python\n",
    "- do that efficiently using batches\n",
    "- illustrate the results for randomly initialized neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports: only numpy and matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the \"numpy\" library for linear algebra\n",
    "\n",
    "# In the lecture videos, I do this:\n",
    "#\n",
    "# from numpy import *\n",
    "#\n",
    "# WARNING: It is generally considered bad\n",
    "# programming style to \"import *\", as it\n",
    "# can lead to confusion. For me, I\n",
    "#  (1) ALWAYS import numpy\n",
    "#  (2) NEVER import any other package in this * way\n",
    "# Therefore, there is never confusion for me, and\n",
    "# it makes my code a bit more readable (for me).\n",
    "# However, since 99% of people are using the \n",
    "# syntax \"import numpy as np\" and then\n",
    "# access \"np.exp()\" etc., you\n",
    "# should probably also use \"np\" once you start\n",
    "# exchanging code with others. I convert\n",
    "# back to the np. syntax when I turn my\n",
    "# converged code into a module.\n",
    "#\n",
    "# It is apparently officially accepted to explicitly\n",
    "# list all the functions you need from numpy:\n",
    "\n",
    "from numpy import array, zeros, exp, random, dot, shape, reshape, meshgrid, linspace\n",
    "\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.dpi']=300 # highres display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A very simple neural network (no hidden layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A network with N0 input neurons and N1 output neurons (no hidden layer)\n",
    "\n",
    "$$y^{\\rm out}_j = f(\\sum_k w_{jk} y^{\\rm in}_k + b_j)$$\n",
    "\n",
    "where $w$ is the weight matrix, $b$ is the bias vector, and $f$ would be the activation function (e.g. the sigmoid here), which is applied independently for each $j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N0=3 # input layer size\n",
    "N1=2 # output layer size\n",
    "\n",
    "# initialize random weights: array dimensions N1xN0\n",
    "w=random.uniform(low=-1,high=+1,size=(N1,N0))\n",
    "# initialize random biases: N1 vector\n",
    "b=random.uniform(low=-1,high=+1,size=N1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input values\n",
    "y_in=array([0.2,0.4,-0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate network by hand, in two steps\n",
    "z=dot(w,y_in)+b # result: the vector of 'z' values, length N1\n",
    "y_out=1/(1+exp(-z)) # the 'sigmoid' function (applied elementwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network input y_in: [ 0.2  0.4 -0.1]\n",
      "weights w: [[-0.10438302  0.90536501  0.34479841]\n",
      " [ 0.14795581  0.96366737 -0.78710148]]\n",
      "bias vector b: [0.62298635 0.49342234]\n",
      "linear superposition z: [0.92977591 0.9871906 ]\n",
      "network output y_out: [0.71702982 0.72853266]\n"
     ]
    }
   ],
   "source": [
    "print(\"network input y_in:\", y_in)\n",
    "print(\"weights w:\", w)\n",
    "print(\"bias vector b:\", b)\n",
    "print(\"linear superposition z:\", z)\n",
    "print(\"network output y_out:\", y_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize network result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still stay with the simple network, but define a function that evaluates the network, and visualize the  output for various inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that applies the network\n",
    "def apply_net(y_in):\n",
    "    global w, b\n",
    "    \n",
    "    z=dot(w,y_in)+b    \n",
    "    return(1/(1+exp(-z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N0=2 # input layer size\n",
    "N1=1 # output layer size\n",
    "\n",
    "w=random.uniform(low=-10,high=+10,size=(N1,N0)) # random weights: N1xN0\n",
    "b=random.uniform(low=-1,high=+1,size=N1) # biases: N1 vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07188659])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_net([0.8,0.3]) # a simple test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: this is NOT the most efficient way to do this! (but simple)\n",
    "# We will later learn to use array syntax efficiently\n",
    "\n",
    "M=50 # will create picture of size MxM\n",
    "y_out=zeros([M,M]) # array MxM, to hold the result\n",
    "\n",
    "for j1 in range(M):\n",
    "    for j2 in range(M):\n",
    "        # out of these integer indices, generate\n",
    "        # two values in the range -0.5...0.5\n",
    "        # and then apply the network to those two\n",
    "        # input values\n",
    "        value0=float(j1)/M-0.5\n",
    "        value1=float(j2)/M-0.5\n",
    "        y_out[j1,j2]=apply_net([value0,value1])[0]"
   ]
  }
